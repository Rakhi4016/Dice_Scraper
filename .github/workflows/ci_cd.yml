name: CI Pipeline for Scraping Job

on:
  push:
    branches:
      - main
      - dev
      - 'feature/**'
  workflow_dispatch:

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest  # Use a Linux-based runner

    env:
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      # Step 1: Checkout the code from your repository
      - name: Checkout the code
        uses: actions/checkout@v3

      # Step 2: Set up Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      # Step 3: Install the dependencies from requirements.txt
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      # Step 4: Set DATE_TAG environment variable
      - name: Set DATE_TAG environment variable
        run: echo "DATE_TAG=$(date +'%Y-%m-%d')" >> $GITHUB_ENV

      # Step 5: Run your main script
      - name: Run scraper
        run: |
          python main.py

      # Step 6: Upload CSV to Workspace (Note: This command assumes it's a notebook, not a CSV)
      - name: Upload CSV to Workspace
        run: |
          FILE_NAME="scraped_jobs_${{ env.DATE_TAG }}.csv"
          databricks workspace import --language PYTHON --format SOURCE "$FILE_NAME" "/Shared/$FILE_NAME"
