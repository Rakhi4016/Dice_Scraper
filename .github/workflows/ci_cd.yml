name: CI Pipeline for Scraping Job

on:
  push:
    branches:
      - main
      - dev
      - 'feature/**'
  workflow_dispatch:

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest  # Use a Linux-based runner

    env:
      SF_USER: ${{ secrets.SF_USER }}
      SF_PASSWORD: ${{ secrets.SF_PASSWORD }}
      SF_ACCOUNT: ${{ secrets.SF_ACCOUNT }}
      SF_WAREHOUSE: ${{ secrets.SF_WAREHOUSE }}
      SF_ROLE: ${{ secrets.SF_ROLE }}
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      - name: ğŸ“¥ Checkout code
        uses: actions/checkout@v3

      - name: ğŸ Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.12

      - name: ğŸ“¦ Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install \
            requests \
            pandas \
            selenium>=4.8.0 \
            snowflake-connector-python \
            python-dotenv \
            databricks-cli

      - name: ğŸ–¥ï¸ Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl unzip
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt install -y ./google-chrome-stable_current_amd64.deb

      
      - name: ğŸ” Test Snowflake Connection
        run: |
          python <<EOF
          import os
          import snowflake.connector

          conn = snowflake.connector.connect(
              user=os.getenv('SF_USER'),
              password=os.getenv('SF_PASSWORD'),
              account=os.getenv('SF_ACCOUNT'),
              warehouse=os.getenv('SF_WAREHOUSE'),
              role=os.getenv('SF_ROLE')
          )
          print('âœ… Snowflake connection successful!')
          conn.close()
          EOF

      - name: ğŸš€ Run scraper and upload CSV to stage
        run: python main.py

      - name: Upload CSV to Workspace
        run: |
          FILE_NAME="scraped_jobs_${{ env.DATE_TAG }}.csv"
          databricks workspace import --language PYTHON --format SOURCE "$FILE_NAME" "/Shared/$FILE_NAME"  
