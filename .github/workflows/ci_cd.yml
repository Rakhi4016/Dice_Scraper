name: CI Pipeline for Scraping Job

on:
  push:
    branches:
      - main
      - dev
      - 'feature/**'
  workflow_dispatch:
  schedule:
    # Runs every day at 23:00 UTC (which is 7:00 PM Eastern Time during DST)
    - cron: '0 23 * * *'

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest  

    env:
      SF_USER: ${{ secrets.SF_USER }}
      SF_PASSWORD: ${{ secrets.SF_PASSWORD }}
      SF_ACCOUNT: ${{ secrets.SF_ACCOUNT }}
      SF_WAREHOUSE: ${{ secrets.SF_WAREHOUSE }}
      SF_ROLE: ${{ secrets.SF_ROLE }}
      DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
      DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

    steps:
      - name:  Checkout code
        uses: actions/checkout@v3

      - name:  Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.12

      - name:  Install Python dependencies
        run: |
          pip install --upgrade pip
          pip install \
            requests \
            pandas \
            selenium>=4.8.0 \
            snowflake-connector-python \
            python-dotenv \
            databricks-cli \
            awscli

      - name:  Install Chrome
        run: |
          sudo apt-get update
          sudo apt-get install -y wget curl unzip
          wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt install -y ./google-chrome-stable_current_amd64.deb

      
      - name:  Test Snowflake Connection
        run: |
          python <<EOF
          import os
          import snowflake.connector

          conn = snowflake.connector.connect(
              user=os.getenv('SF_USER'),
              password=os.getenv('SF_PASSWORD'),
              account=os.getenv('SF_ACCOUNT'),
              warehouse=os.getenv('SF_WAREHOUSE'),
              role=os.getenv('SF_ROLE')
          )
          print(' Snowflake connection successful!')
          conn.close()
          EOF

      - name:  Run scraper and upload CSV to stage
        run: python main.py

      - name:  Set DATE_TAG
        run: echo "DATE_TAG=$(date +'%Y-%m-%d')" >> $GITHUB_ENV  

      - name: Upload CSV to Workspace
        run: |
          FILE_NAME="scraped_jobs_${{ env.DATE_TAG }}.csv"
          databricks workspace import --language PYTHON --format SOURCE "$FILE_NAME" "/Shared/$FILE_NAME"  

      - name:  Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v2
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_REGION }} 

      - name:  Upload CSV to S3
        run: |
          FILE_NAME="scraped_jobs_${{ env.DATE_TAG }}.csv"
          aws s3 cp "$FILE_NAME" "s3://my-dice-scraper/input/$FILE_NAME"    
